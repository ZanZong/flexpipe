_BASE_: "../base_model_bert_l12_h768.yaml"


TASKS:

  
  
  -
    NAME: mscoco_retrieve
    DATASETS:
      TRAIN: 'ImageTextPairDataset'
      TEST: 'ImageTextPairDataset'
      TASK_TYPE: 'image_retrieval'
      DATASET_NAME: 'MSCOCO'
    DATALOADER:
      TRAIN_BATCH_SIZE: 256
      TEST_BATCH_SIZE: 64
      NUM_WORKERS: 2
      FEATS_FOLDER: 'open_source_dataset/mscoco_dataset/coco_origin'
      ANNO_FOLDER:  'open_source_dataset/mscoco_dataset/new_annotations'
      S3_PATH: 's3://coco/'
      SEQ_PER_SAMPLE:  1
      CACHE_MODE: True
      CIRCULAR_CACHE_MODE: False
      ZIP_MODE: False
      CACHE_ORIGIN_IMAGE: False
      RANDOM_CAPTION: False
      AS_NUMPY_AS_POSSIBLE: False
      SAMPLING_WEIGHT: 0.5
      TRANSFORM: 'clip_transforms'
      DATA_PERCENTAGE: 0.01
    MODEL:
      MAX_SEQ_LEN: 30
      TEMP_NAME: logit_scale_retrieve
    LOSSES:
      NAMES: ['LabelSmoothingCrossEntropy', 'Accuracy']
      LABELSMOOTHING: 0.1
      LOSS_WEIGHT: 0.5
      REDUCTION: 'mean'
    INFERENCE:
      NAME: 'RetrievalEvaler'
      GENERATION_MODE: False

  


ENGINE:
  NAME: 'UnifiedTrainer'
 
MODEL:
  META_ARCHITECTURE: 'MultiTaskTransformerEncoder'
  ENCODER: 'UnifiedBertEncoder'

  SHARE_LAYERNORM: True
  BERT:
    NORMALIZE_DECISION: "BERTPre" 
    DROP_PATH_PROB: 0.2
    DROP_PATH_PROB_FIXED: True


  MODEL_EMA: False
  MODEL_EMA_DECAY: 0.9999

  MAEParamsInit: True
  POSEMBEDFIX: True


  IMG_INPUT_SIZE: 224
  PATCH_SIZE: 16
  
  POSEMBED_SCALE: !!python/object/apply:eval ["160/224"]
  CHECKPOINT_FILETER: False 
  OLD_CHECKPONT: True 

  LAYER_SCALE: True 
  LAYER_SCALE_INIT: 1e-3

  # prompt
  PROMPT: True
  PROMPT_PARAM: ["s_token_bias", "norm", "prompt_embed", "deep_prompt_embedding", "fc_prompt", "similarity_weight", "ln_post"] # ["s_token_bias", "LayerNorm", "prompt_embed", "prompt_fc", "similarity_weight"]
  FC_PROMPT: False


  # #################################### prompt embedding ####################################
  PROMPT_EMBED: #### activated only when the
    NAME: 'PrefixPromptEmbedding'
    ACTIVATION: 'none'
    ELU_ALPHA: 0.5
    USE_NORM: False
    DROPOUT: 0.0
    WITH_POS: False
    INPUT_PROMPT: False 
    TARGET_PROMPT: False
    DEEP_PROMPT: True 
    TARGET_DEEP_PROMPT: True 
    SHARE_DEEP_PROMPT: False 
    PROMPT_LENGTH: 10
    TARGET_PROMPT_LENGTH: 1
    INPUT_DEEP_PROMPT_LENGTH: 10
    TARGET_DEEP_PROMPT_LENGTH: 10
    LABLE_PROMPT: False
    LABEL_SIZE: 1000


DATALOADER:
  USE_WEIGHTED_SAMPLER: True
  UNIFIED_DATASET: True 
  NUM_WORKERS: 16

  PADDING_TO_MAX: False # True for debugging or token moe with distributed moe 


  
####################################### Optimizer #######################################
SOLVER:
  NAME: 'Adam'
  TORCH_OPTIMIZER: True
  PARAMS_SEPERATE: True
  # PARAMS_GROUP: True
  # EPOCH: 1
  MAX_ITER: 500
  CHECKPOINT_PERIOD: 50000
  EVAL_PERIOD: 50
  BASE_LR: 0.001
  BIAS_LR_FACTOR: 1.0
  WEIGHT_DECAY: 0.0
  WEIGHT_DECAY_NORM: 0.0
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_EMBEDDING: 0.0
  MOMENTUM: 0.9
  DAMPENING: 0.0
  NESTEROV: 0.0
  BETAS: [0.9, 0.95]
  EPS: 1e-6
  GRAD_CLIP: 0.1
  GRAD_CLIP_TYPE: 'norm'
  ACCUM_ITER: 0
  AMP_FP16: True
  APEX_FP16: False # dangerous

  WRITE_PERIOD: 25
  MIN_LOSS_SCLE: 2048.0
  # BF16: False # True
  # ZEROSTAGE: 2

  LOSS_SCALE_WINDOW: 200





  
####################################### lr scheduler ####################################### 
LR_SCHEDULER:
  NAME: 'WarmupCosine'
  WARMUP: 50
  MIN_LR: 0.000001

